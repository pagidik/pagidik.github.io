<!DOCTYPE html>
<html lang="">
  <head>
    <meta charset="utf-8">
    <title></title>
  </head>
  <body>
    <header></header>
    <main></main>
    <footer></footer>
  </body>

  <!DOCTYPE HTML>
  <!--
      Massively by HTML5 UP
      html5up.net | @ajlkn
      Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
  <html>
      <head>
          <title>Projects - Kishore's Portfolio</title>
          <meta charset="utf-8" />
          <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
          <link rel="stylesheet" href="assets/css/main.css" />
          <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
          <style>
                  h1 {
                  font-size: 1rem;
                  }
                  h2 {
                  font-size: 1rem;
                  }
                  p {
                  font-size: 1rem;
                  line-height: 1.5;
                  }
          </style>
      </head>
      <body class="is-preload">
  
          <!-- Wrapper -->
              <div id="wrapper">
  
                  <!-- Header -->
                      <header id="header">
                          <a href="Projects.html" class="logo">Projects</a>
                      </header>
  
                  <!-- Nav -->
                      <nav id="nav">
                          <ul class="links">
                              <li><a href="index.html">About</a></li>
                              <li class="active"><a href="projects.html">Projects</a></li>
                              <li><a href="/assets/Kishore_Resume.docx" target="_blank">Resume</a></li>
                          </ul>
                          <ul class="icons">
                              <li><a href="https://www.linkedin.com/in/kishore005/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
                              <li><a href="https://github.com/pagidik" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                          </ul>
                      </nav>
  
                      <!-- Main -->
                      <div id="main">
  
                          <!-- Post -->
                              <section class="post">
                                  <header class="major">
                                      <h2>3D Object Detection on Lidar Point Cloud</h2>
                                  </header>
                                  
                                  <!-- Text stuff -->
                                  <h2 id="introduction">Introduction</h2>
                                  <p>Autonomous vehicles and robotics applications rely on accurate perception of their surroundings in order to navigate and perform tasks safely and effectively. One key aspect of perception is the ability to detect and classify objects in the environment. Traditional 2D object detection methods, which rely on image data from cameras, can be affected by occlusions and partial visibility of objects in complex and dynamic environments. Lidar (Light Detection and Ranging) sensors offer an alternative approach for 3D object detection, as they generate 3D point cloud data using lasers that can provide more accurate localization and orientation estimation of objects.</p>
                                  <p>In this paper, we propose a method for 3D object detection using lidar data in autonomous vehicles and robotics applications. Our approach involves representing the point cloud data in voxels, which are 3D grids that partition the data into small cubic cells, and a bird’s eye view (BEV) representation, which projects the 3D data onto a 2D plane. We use a combination of UNet, a deep learning approach commonly used for image segmentation, and these representations to effectively process and differentiate objects in cluttered scenes.</p>
                                  <p>We evaluate our approach on the Lyft dataset, which contains real-world urban driving scenarios with both lidar and camera data. The dataset includes a variety of objects, such as vehicles, pedestrians, and traffic signs, in a range of challenging conditions, including occlusions and partial visibility. We show that our method achieves strong performance in 3D object detection, outperforming several baseline approaches.</p>
                                  <p>Overall, our approach has the potential to be applied to a wide range of autonomous vehicles and robotics applications, where the accurate perception of the environment is critical for safe and effective operation.</p>
                                  <a href="#" class="image fit"><img src="images/3dobject/bev.png" alt="" /></a>
                                  
                                  <h1 id="recent-work">Recent Work</h1>
                                  <p>3D object detection using lidar data has received significant attention in recent years, with numerous approaches being proposed to address the challenges of high-dimensional data and complex environments. Some of the notable recent works in this field include:</p>
                                  <p>Qi et al. [1] proposed PointNet, a deep learning approach for 3D object classification and segmentation using raw point cloud data. PointNet uses a symmetric architecture with multi-layer perceptrons (MLPs) to learn global features from the point cloud. This approach is point-based and does not utilize voxels or a BEV representation.</p>
                                  <p>Xiao et al. [2] proposed a 3D object detection method based on a region proposal network (RPN) and a 3D convolutional neural network (CNN). The RPN generates proposals for 3D bounding boxes in the point cloud, which are then refined using the 3D CNN. This approach is point-based and does not utilize voxels or a BEV representation.</p>
                                  <p>Yang et al. [3] proposed a 3D object detection method based on a combination of point-based and voxel-based approaches. The method first converts the point cloud data into a voxel grid and then uses a point-based network to process the voxels and predict object classes and locations. This approach utilizes voxels but does not use a BEV representation.</p>
                                  <p>In contrast, our proposed approach for 3D object detection using UNet, voxels, and BEV representation</p>
                                  <h1 id="dataset">Dataset</h1>
                                  <p>In the data exploration, the goal is to understand and analyze the dataset for self-driving cars. The dataset includes both image data in the form of JPEGs and LiDAR data, which is used to create 3D representations of the car’s surroundings. The organization of the data into "scenes" and "samples" allows for a clear understanding of the context in which the data was collected and the annotated objects within the samples provide important information for object recognition and tracking.</p>
                                  <p>The ego pose data, which includes the poses of the vehicle at specific timestamps, is also important for understanding the movement and positioning of the car within its environment. This information can be used for localization and mapping, as well as trajectory planning and control. The map data, which is stored as binary semantic masks from a top-down view, can also be useful for localization and understanding the layout of the car’s surroundings.</p>
                                  <a href="#" class="image fit"><img src="images/3dobject/classescount.png" alt="" /></a>
                                  
                                  <h1 id="method">Method</h1>
                                  <h2 id="pre-processing">Pre-Processing</h2>
                                  <p>Voxelization is the process of discretizing a continuous space into a set of discrete 3D cells or "voxels". In the case of lidar point cloud data, voxelization is used to create a 3D grid representation of the data, where each voxel contains a certain number of points within its volume.</p>
                                  <p>To voxelize the point cloud data in this paper, we constructed a 4x4 transformation matrix that can be used to transform the points into the desired voxel space. The transformation matrix is constructed such that the voxel grid is centered at the origin (0,0,0) and the voxels have the specified size. An optional offset can be provided for each axis in world coordinates, which is useful for adjusting the position of the voxel grid in the case of lidar data where the sensor is not at the origin.</p>
                                  <p>In the above expressions (1) and (2) t is the translation vector, tm is the transformation matrix which is initially an 4 x 4 identity matrix. Once the transformation matrix is created, it is used to transform the points into the voxel space using points and the transformation matrix using eq (3). The transformed points are now in the same coordinate system as the voxel grid and can be used to populate the voxels.</p>
                                  <p>After voxelization, the point cloud data was converted into a bird’s eye view (BEV) representation using voxelized points, voxel volume shape, and voxel size. The points in the BEV representation, where the x and y coordinates correspond to the indices of the voxels and the z coordinate corresponds to the height of the points are generated. An optional z-offset can also be provided to account for the height of the sensor.</p>
                                  <p>BEV representation is a top-down view of the data, where the x and y axes correspond to the lateral dimensions of the scene and the z-axis corresponds to the height. BEV representation is often used for 2D object detection tasks, as it simplifies the data and reduces the number of dimensions.</p>
                                  <p>After voxelization and BEV conversion, the data is ready for object detection. We also preprocessed the point cloud data to generate training and validation datasets. This involved dividing the point cloud data into two sets, one for training the neural network and one for evaluating its performance. The datasets were prepared in a suitable format for input to the neural network and were stored as TensorFlow records for efficient loading during the training process.</p>
                                  <h2 id="network">Network</h2>
                                  <p>The pre-processed point cloud data was then used to train a neural network using a supervised learning approach using UNet architecture. The UNet architecture was implemented using the TensorFlow deep learning framework and was trained to perform object detection and semantic segmentation. The task was defined using a set of annotated examples from the training dataset, where each example consisted of an input BEV representation and a corresponding label indicating the presence and location of objects in the scene.</p>
                                  <p>During training, the network was optimized by minimizing a loss function that measured the difference between the predicted labels and the ground truth labels. The loss function was defined using a custom implementation of the sparse categorical cross-entropy loss, which is well-suited for multi-class classification tasks. In this particular implementation, the labels are first one-hot encoded. The one-hot encoded labels and the predictions are then used to compute the categorical cross-entropy loss between the two.</p>
                                  <p>The major difference between this custom loss function and the standard sparse categorical cross-entropy loss is that it allows the use of class weights. Class weights are used to assign different levels of importance or weight to different classes in the dataset. This can be useful in cases where the classes are imbalanced, i.e., there are significantly more examples of one class compared to others. By assigning higher weights to under-represented classes, the model can be encouraged to pay more attention to these classes and improve its performance on them. In this implementation, the class weights are passed as a dictionary and are converted into a tensor before being used in the loss calculation.</p>
                                  <p>The optimization process was performed using the backpropagation algorithm and the Adam optimizer, which adjusts the network parameters through gradient descent.</p>
                                  <h1 id="evaluation-and-visualization">Evaluation and Visualization</h1>
                                  <p>To evaluate the performance of the trained neural network, we generated predictions for random examples from the training and validation datasets. We compared the predictions to the ground truth labels to assess the accuracy and generalization of the network. We also visualized the input data, ground truth labels, and network predictions to gain insight into the network’s behavior and identify areas for improvement.</p>
                                  <p>To facilitate visualization, we reversed the preprocessing steps to reconstruct the original point cloud data from the voxelized and BEV-transformed representation. We used this data to generate 2D images using a variety of rendering techniques, such as color-coding the point cloud based on the height dimension or overlaying the predicted labels on the point cloud. These images allowed us to qualitatively assess the quality of the predictions and the overall performance of the network.</p>
                                  <p>In summary, our method combines voxelization and BEV representation with supervised learning to effectively process and analyze 3D lidar point cloud data and train a neural network for a specific task. The resulting system is flexible and can be easily adapted to different applications by modifying the training data and the network architecture.</p>
                                  <h1 id="discussion-and-summary">Discussion and summary</h1>
                                  <p>In this paper, we have presented a method for 3D object detection using lidar data in autonomous vehicles and robotics applications. Our approach involves representing the point cloud data in voxels and a bird’s eye view (BEV) representation and using a combination of UNet and these representations to effectively process and differentiate objects in cluttered scenes. We have evaluated our approach on the Lyft dataset and showed that it outperforms several baseline approaches in 3D object detection, achieving strong performance in real-world urban driving scenarios with a variety of objects and challenging conditions.</p>
                                  <p>The use of lidar data in 3D object detection offers several advantages over traditional 2D methods that rely on image data. Lidar sensors generate 3D point cloud data using lasers, which allows for improved localization and orientation estimation of objects in dynamic environments. In addition, lidar data is not affected by occlusions and partial visibility in the same way that image data is, making it a more reliable source of information for perception in complex environments.</p>
                                  <p>There are several potential applications of our approach in the field of autonomous vehicles and robotics. In autonomous vehicles, accurate 3D object detection is critical for safe navigation and decision-making. Our method has the potential to be integrated into the perception systems of autonomous vehicles, allowing them to perceive and respond to their surroundings. In robotics, 3D object detection can be used for tasks such as grasping and manipulation of objects, as well as for navigation in dynamic environments.</p>
                                  <p>There are several directions for future work that can build upon the foundations presented in this paper. One potential avenue is to explore the use of other deep learning architectures, such as Mask R-CNN, for 3D object detection with lidar data. Another possibility is to investigate the use of multi-modal data, combining lidar and image data, for even more robust object detection. Additionally, further studies could be conducted to evaluate the performance of our approach in different types of environments and tasks.</p>
              
                                  </section>
  
                      </div>
  
              </div>
  
          <!-- Scripts -->
              <script src="assets/js/jquery.min.js"></script>
              <script src="assets/js/jquery.scrollex.min.js"></script>
              <script src="assets/js/jquery.scrolly.min.js"></script>
              <script src="assets/js/browser.min.js"></script>
              <script src="assets/js/breakpoints.min.js"></script>
              <script src="assets/js/util.js"></script>
              <script src="assets/js/main.js"></script>
  
      </body>
  </html></html>
